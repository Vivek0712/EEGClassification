{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"finalv2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPDeJthJzBV2SsJyatpOqcf"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"NSZjJtuUYABt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"ade7e9ae-b3da-4da0-b1d1-e1199ba3c09e","executionInfo":{"status":"ok","timestamp":1587636600257,"user_tz":-330,"elapsed":237960,"user":{"displayName":"Vivek Raja P S","photoUrl":"","userId":"10072481135673478114"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YKQt3zgziQNA","colab_type":"text"},"source":["Preprocessing steps done:\n","\n","1. Replacing the Nan with mean\n","2. Experimented with different ways to preprocess the data (no processing, standardisation, normlization, scaling, minmaxscaling, maxabs, quartile, minmax + sqrt, quartile+maxabs)\n","\n","Hyperparameter Tuning:\n","\n","1. Dropout layer values\n","2. Adding/removal of layers\n","3. Learning rate, optimizers \n","\n","Cross Validation: (0.1,0.2,0.3,0.4,0.5,0.6,0.65,0.7,0.75,0.8,0.85)\n","Train test Split : 80:20 (Best)\n","\n","Models:\n","\n","Basic Classification Models - Poor Results\n","\n","Deep Learning Models\n","1. LSTM  (maxabs preprocessing) \n","\n","Training Score\n","F1 Score:  0.6396809608016505\n","Accuracy Score:  0.6404494382022472\n","Testing Score\n","F1 Score:  0.3297208538587849\n","Accuracy Score:  0.34328358208955223\n","\n","2. CNN (best works when no processing of data)\n","Training Score\n","F1 Score:  0.8683120136140271\n","Accuracy Score:  0.8726591760299626\n","Testing Score\n","F1 Score:  0.32180385575971315\n","Accuracy Score:  0.3283582089552239\n","\n","3. LSTM + CNN - Poor Results\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"H8IxnosAYeCE","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_boston\n","import pandas as pd\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import statsmodels.api as sm\n","%matplotlib inline\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.feature_selection import RFE\n","from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n","from pandas import read_csv\n","from pandas import DataFrame\n","from pandas import Grouper\n","from matplotlib import pyplot\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import accuracy_score\n","from sklearn import preprocessing\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler\n","from keras.utils import to_categorical\n","from sklearn import preprocessing\n","from numpy import sqrt\n","from numpy import log\n","from scipy.stats import boxcox\n","from numpy import mean,std"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NL2xN9HbY7tQ","colab_type":"code","colab":{}},"source":["df = pd.read_csv(r'/content/drive/My Drive/Manasa_ML/finalcombined.csv')\n","df.head()\n","df.fillna(df.mean(), inplace=True)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vKikyGYLZAGL","colab_type":"code","colab":{}},"source":["\n","\n","X = df.drop(['1'],axis=1)\n","y=df.loc[:,'1']\n","X = X.reset_index(drop=True)\n","y = y.reset_index(drop=True)\n","# y.loc[100] = '4' ####\n","y = y.astype(int)\n","y = to_categorical(y) \n","X_train , X_test , y_train , y_test = train_test_split(X,y,random_state=16,test_size=0.2)\n","\n","Xtrain_t = X_train.T\n","Xtest_t = X_test.T\n","\n","# normalize\n","# Xtrain_n=preprocessing.normalize(Xtrain_t)\n","# Xtest_n=preprocessing.normalize(Xtest_t)\n","# X_train_n = np.reshape(Xtrain_n, (Xtrain_n.shape[1],Xtrain_n.shape[0],1))\n","# X_test_n = np.reshape(Xtest_n, (Xtest_n.shape[1],Xtrain_n.shape[0],1))\n","\n","\n","\n","#nopreprocessing\n","# Xtrain_n=np.asarray(Xtrain_t)\n","# Xtest_n=np.asarray(Xtest_t)\n","# X_train_n = np.reshape(Xtrain_n, (Xtrain_n.shape[1],Xtrain_n.shape[0],1))\n","# X_test_n = np.reshape(Xtest_n, (Xtest_n.shape[1],Xtrain_n.shape[0],1))\n","\n","\n","#Standard\n","# scaler = StandardScaler()\n","# Xtrain_n=scaler.fit_transform(Xtrain_t)\n","# Xtest_n=scaler.fit_transform(Xtest_t)\n","# X_train_n = np.reshape(Xtrain_n, (Xtrain_n.shape[1],Xtrain_n.shape[0],1))\n","# X_test_n = np.reshape(Xtest_n, (Xtest_n.shape[1],Xtrain_n.shape[0],1))\n","\n","\n","#Scaling\n","# Xtrain_n=preprocessing.scale(Xtrain_t)\n","# Xtest_n=preprocessing.scale(Xtest_t)\n","# X_train_n = np.reshape(Xtrain_n, (Xtrain_n.shape[1],Xtrain_n.shape[0],1))\n","# X_test_n = np.reshape(Xtest_n, (Xtest_n.shape[1],Xtrain_n.shape[0],1))\n","\n","# Min max Scaling\n","# scaler =preprocessing.MinMaxScaler()\n","# Xtrain_n=scaler.fit_transform(Xtrain_t)\n","# Xtest_n=scaler.fit_transform(Xtest_t)\n","# X_train_n = np.reshape(Xtrain_n, (Xtrain_n.shape[1],Xtrain_n.shape[0],1))\n","# X_test_n = np.reshape(Xtest_n, (Xtest_n.shape[1],Xtrain_n.shape[0],1))\n","\n","\n","# # minmax + SQRT\n","# scaler =preprocessing.MinMaxScaler()\n","# Xtrain_n=scaler.fit_transform(Xtrain_t)\n","# Xtest_n=scaler.fit_transform(Xtest_t)\n","# Xtrain_n=sqrt(Xtrain_n)\n","# Xtest_n=sqrt(Xtest_n)\n","# X_train_n = np.reshape(Xtrain_n, (Xtrain_n.shape[1],Xtrain_n.shape[0],1))\n","# X_test_n = np.reshape(Xtest_n, (Xtest_n.shape[1],Xtrain_n.shape[0],1))\n","\n","# max abs Scaling\n","# scaler =preprocessing.MaxAbsScaler()\n","# Xtrain_n=scaler.fit_transform(Xtrain_t)\n","# Xtest_n=scaler.fit_transform(Xtest_t)\n","# X_train_n = np.reshape(Xtrain_n, (Xtrain_n.shape[1],Xtrain_n.shape[0],1))\n","# X_test_n = np.reshape(Xtest_n, (Xtest_n.shape[1],Xtrain_n.shape[0],1))\n","\n","#maxabs + Quarantile transformation\n","# scaler =preprocessing.MaxAbsScaler()\n","# Xtrain_n=scaler.fit_transform(Xtrain_t)\n","# Xtest_n=scaler.fit_transform(Xtest_t)\n","# scaler =preprocessing.QuantileTransformer(random_state=0)\n","# Xtrain_n=scaler.fit_transform(Xtrain_n)\n","# Xtest_n=scaler.fit_transform(Xtest_n)\n","# X_train_n = np.reshape(Xtrain_n, (Xtrain_n.shape[1],Xtrain_n.shape[0],1))\n","# X_test_n = np.reshape(Xtest_n, (Xtest_n.shape[1],Xtrain_n.shape[0],1))\n","\n","#ipsative std\n","# mn = mean(Xtrain_t)\n","# sd = std(Xtrain_t)\n","# Xtrain_n = (Xtrain_t-mn)/sd\n","# mn = mean(Xtest_t)\n","# sd = std(Xtest_t)\n","# Xtest_n = (Xtest_t-mn)/sd\n","# X_train_n = Xtrain_n\n","# X_test_n = Xtest_n\n","# print(Xtrain_n.shape,Xtest_n.shape)\n","# X_train_n = np.reshape(Xtrain_n.values, (Xtrain_n.shape[1],Xtrain_n.shape[0],1))\n","# X_test_n = np.reshape(Xtest_n.values, (Xtest_n.shape[1],Xtrain_n.shape[0],1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eoKYMnFeu5qa","colab_type":"code","colab":{}},"source":["from keras.layers import Conv1D, MaxPooling1D,GlobalAveragePooling1D\n","\n","model = Sequential()\n","model.add(Conv1D(40, 10, strides=2, padding='same', activation='relu',input_shape=(Xtrain_n.shape[0],1) ))\n","model.add(Dropout(0.55))\n","model.add(MaxPooling1D(3))\n","model.add(Conv1D(40, 5, strides=2, padding='same', activation='relu'))\n","model.add(Dropout(0.1))\n","model.add(MaxPooling1D(3))\n","model.add(Conv1D(40, 4, strides=1, padding='same', activation='relu'))\n","model.add(Dropout(0.15))\n","model.add(MaxPooling1D(3))\n","model.add(GlobalAveragePooling1D())\n","model.add(Dense(50, activation='relu'))\n","model.add(Dropout(0.65))\n","model.add(Dense(5, activation='softmax'))\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","history = model.fit(X_train_n,y_train, validation_data=(X_test_n,y_test),epochs=100)\n","y_pred = model.predict(X_test_n)\n","ypred = np.argmax(y_pred, axis=1)\n","y_predt = model.predict(X_train_n)\n","ypred_train = np.argmax(y_predt, axis=1)\n","\n","print(\"Training Score\")\n","print(\"F1 Score: \",f1_score(np.argmax(y_train, axis=1), ypred_train, average='macro'))\n","print(\"Accuracy Score: \",accuracy_score(np.argmax(y_train, axis=1), ypred_train))\n","print(\"Testing Score\")\n","print(\"F1 Score: \",f1_score(np.argmax(y_test, axis=1), ypred, average='macro'))\n","print(\"Accuracy Score: \",accuracy_score(np.argmax(y_test, axis=1), ypred))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vmbOMlk5g6kB","colab_type":"code","colab":{}},"source":["\n","\n","X = df.drop(['1'],axis=1)\n","y=df.loc[:,'1']\n","X = X.reset_index(drop=True)\n","y = y.reset_index(drop=True)\n","# y.loc[100] = '4' ####\n","y = y.astype(int)\n","y = to_categorical(y) \n","X_train , X_test , y_train , y_test = train_test_split(X,y,random_state=16,test_size=0.2)\n","\n","Xtrain_t = X_train.T\n","Xtest_t = X_test.T\n","\n","#nopreprocessing\n","# Xtrain_n=np.asarray(Xtrain_t)\n","# Xtest_n=np.asarray(Xtest_t)\n","# X_train_n = np.reshape(Xtrain_n, (Xtrain_n.shape[1],Xtrain_n.shape[0],1))\n","# X_test_n = np.reshape(Xtest_n, (Xtest_n.shape[1],Xtrain_n.shape[0],1))\n","#Standard\n","# scaler = StandardScaler()\n","# Xtrain_n=scaler.fit_transform(Xtrain_t)\n","# Xtest_n=scaler.fit_transform(Xtest_t)\n","# X_train_n = np.reshape(Xtrain_n, (Xtrain_n.shape[1],Xtrain_n.shape[0],1))\n","# X_test_n = np.reshape(Xtest_n, (Xtest_n.shape[1],Xtrain_n.shape[0],1))\n","\n","\n","#Scaling\n","# Xtrain_n=preprocessing.scale(Xtrain_t)\n","# Xtest_n=preprocessing.scale(Xtest_t)\n","# X_train_n = np.reshape(Xtrain_n, (Xtrain_n.shape[1],Xtrain_n.shape[0],1))\n","# X_test_n = np.reshape(Xtest_n, (Xtest_n.shape[1],Xtrain_n.shape[0],1))\n","\n","# Min max Scaling\n","# scaler =preprocessing.MinMaxScaler()\n","# Xtrain_n=scaler.fit_transform(Xtrain_t)\n","# Xtest_n=scaler.fit_transform(Xtest_t)\n","# X_train_n = np.reshape(Xtrain_n, (Xtrain_n.shape[1],Xtrain_n.shape[0],1))\n","# X_test_n = np.reshape(Xtest_n, (Xtest_n.shape[1],Xtrain_n.shape[0],1))\n","\n","\n","# # minmax + SQRT\n","# scaler =preprocessing.MinMaxScaler()\n","# Xtrain_n=scaler.fit_transform(Xtrain_t)\n","# Xtest_n=scaler.fit_transform(Xtest_t)\n","# Xtrain_n=sqrt(Xtrain_n)\n","# Xtest_n=sqrt(Xtest_n)\n","# X_train_n = np.reshape(Xtrain_n, (Xtrain_n.shape[1],Xtrain_n.shape[0],1))\n","# X_test_n = np.reshape(Xtest_n, (Xtest_n.shape[1],Xtrain_n.shape[0],1))\n","\n","# max abs Scaling\n","scaler =preprocessing.MaxAbsScaler()\n","Xtrain_n=scaler.fit_transform(Xtrain_t)\n","Xtest_n=scaler.fit_transform(Xtest_t)\n","X_train_n = np.reshape(Xtrain_n, (Xtrain_n.shape[1],Xtrain_n.shape[0],1))\n","X_test_n = np.reshape(Xtest_n, (Xtest_n.shape[1],Xtrain_n.shape[0],1))\n","\n","#maxabs + Quarantile transformation\n","# scaler =preprocessing.MaxAbsScaler()\n","# Xtrain_n=scaler.fit_transform(Xtrain_t)\n","# Xtest_n=scaler.fit_transform(Xtest_t)\n","# scaler =preprocessing.QuantileTransformer(random_state=0)\n","# Xtrain_n=scaler.fit_transform(Xtrain_n)\n","# Xtest_n=scaler.fit_transform(Xtest_n)\n","# X_train_n = np.reshape(Xtrain_n, (Xtrain_n.shape[1],Xtrain_n.shape[0],1))\n","# X_test_n = np.reshape(Xtest_n, (Xtest_n.shape[1],Xtrain_n.shape[0],1))\n","\n","#ipsative std\n","# mn = mean(Xtrain_t)\n","# sd = std(Xtrain_t)\n","# Xtrain_n = (Xtrain_t-mn)/sd\n","# mn = mean(Xtest_t)\n","# sd = std(Xtest_t)\n","# Xtest_n = (Xtest_t-mn)/sd\n","# X_train_n = Xtrain_n\n","# X_test_n = Xtest_n\n","# print(Xtrain_n.shape,Xtest_n.shape)\n","# X_train_n = np.reshape(Xtrain_n.values, (Xtrain_n.shape[1],Xtrain_n.shape[0],1))\n","# X_test_n = np.reshape(Xtest_n.values, (Xtest_n.shape[1],Xtrain_n.shape[0],1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YQW7in92ZD6L","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Convolution2D, MaxPooling1D, LeakyReLU\n","model = Sequential()\n"," \n","model.add(LSTM(units=32, activation='relu', return_sequences=True,input_shape=(Xtrain_n.shape[0],1)))\n","model.add(MaxPooling1D(pool_size=(2)))\n","model.add(Dropout(0.25))\n","model.add(Flatten())\n","model.add(Dense(64, activation='relu'))\n","model.add(LeakyReLU(alpha=0.03))\n","model.add(Dropout(0.45))\n","model.add(Dense(5, activation='softmax'))\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i30d-6P5ZWOa","colab_type":"code","colab":{}},"source":["history = model.fit(X_train_n,y_train, validation_data=(X_test_n,y_test),epochs=100)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6mK6oiPvZbk2","colab_type":"code","colab":{}},"source":["y_pred = model.predict(X_test_n)\n","ypred = np.argmax(y_pred, axis=1)\n","y_predt = model.predict(X_train_n)\n","ypred_train = np.argmax(y_predt, axis=1)\n","\n","print(\"Training Score\")\n","print(\"F1 Score: \",f1_score(np.argmax(y_train, axis=1), ypred_train, average='macro'))\n","print(\"Accuracy Score: \",accuracy_score(np.argmax(y_train, axis=1), ypred_train))\n","print(\"Testing Score\")\n","print(\"F1 Score: \",f1_score(np.argmax(y_test, axis=1), ypred, average='macro'))\n","print(\"Accuracy Score: \",accuracy_score(np.argmax(y_test, axis=1), ypred))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ijmoiPCrG44G","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}